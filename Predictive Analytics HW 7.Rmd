---
title: "Predictive Analytics HW 7"
author: "Rachel Jordan"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Chapter 9, Exercise 3

#### (a)

```{r}
#enter data
X1 <- c(3,2,4,1,2,4,4)
X2 <- c(4,2,4,4,1,3,1)
Y <- c("Red","Red","Red","Red","Blue","Blue","Blue")
toy <- as.data.frame(cbind(X1,X2,Y))
toy$Y <- factor(toy$Y,levels=c("Red","Blue"))

#create "sketch"
require(tidyverse)
ggplot(toy, aes(x=X1, y=X2)) + geom_point(aes(color=Y))
```

#### (b)

Visually I can tell that this line should be X2 = -0.5 + X1 which is the same as 0.5 - X1 + X2 = 0.

```{r}
#sketch
ggplot(toy, aes(x=X1, y=X2)) + geom_point(aes(color=Y)) + geom_abline(slope=1,intercept=-0.5)
```

#### (c)

Classify to red if 0.5 - X1 + X2 \> 0 and classify to blue otherwise. B0 = 0.5, B1 = -1, and B2 = 1.

#### (d)

The margins are created by using -1 and 0 as the intercept. The resulting lines will hit the support vectors.

```{r}
#sketch
ggplot(toy, aes(x=X1, y=X2)) + geom_point(aes(color=Y)) + geom_abline(slope=1,intercept=-0.5) + geom_abline(slope=1, intercept=0, linetype="dotted") + geom_abline(slope=1, intercept=-1,linetype="dotted")
```

#### (e)

The support vectors are the points (2,2), (2,1), (4,4), and (4,3).

#### (f)

The 7th observation, (4,1), is not a support vector and would therefore not affect the maximal marginal hyperplane if moved, UNLESS it was moved to be within the margins or across the hyperplane.

#### (g)

```{r}
#sketch
ggplot(toy, aes(x=X1, y=X2)) + geom_point(aes(color=Y)) + geom_abline(slope=1.1,intercept=-0.5)
```

This is not the optimal separating hyperplane. The equation is X2 = -0.5 + 1.1X1, which is equivalent to 0.5 - 1.1X1 + X2 = 0.

#### (h)

```{r}
#add extra point
X1_2 <- c(3,2,4,1,2,4,4,2)
X2_2 <- c(4,2,4,4,1,3,1,3)
Y_2 <- c("Red","Red","Red","Red","Blue","Blue","Blue","Blue")
toy_2 <- as.data.frame(cbind(X1_2,X2_2,Y_2))
toy_2$Y_2 <- factor(toy_2$Y_2,levels=c("Red","Blue"))

#sketch
ggplot(toy_2, aes(x=X1_2, y=X2_2)) + geom_point(aes(color=Y_2))
```

## Chapter 9, Exercise 4

```{r}
#generate data
set.seed(1354)
X1_3 <- rnorm(100)
X2_3 <- runif(100, min = 0, max = 4)
Y_3 <- ifelse(X1_3**2 + X2_3**2 <= 4, 0, 1)
df <- as.data.frame(cbind(X1_3, X2_3, Y_3))

#plot data
ggplot(df, aes(x=X1_3, y=X2_3)) + geom_point(aes(color=Y_3))

#show that in this setting, a support vector machine with polynomial kernal (with degree greater than 1) or a radial kernel will outperform a support vector classifier on the training data

#create train and test
set.seed(1356)
train_sample <- sample(1:100,size=80,replace=F)
train <- df[train_sample,]
test <- df[-train_sample,]

#support vector classifier
require(e1071)
svc <- svm(factor(Y_3) ~ ., data = train, kernel = "linear")
summary(svc)
tune.out.svc <- tune(svm, factor(Y_3) ~ .,data=df, kernel = "linear", ranges = list(cost = c(0.001, 0.01, 0.1, 1, 5, 10, 100)))
summary(tune.out.svc)
#we want cost to be 1
best_svc <- svm(factor(Y_3) ~ ., data = train, kernel = "linear", cost=1)
summary(best_svc)
#error rate
table(predict = (predict(best_svc,test)), truth = test$Y_3)
#error rate is 3/20 = 15%
plot(best_svc,train) #not too shabby

#support vector machine with polynomial kernel
tune_out_svm_polynomial <- tune(svm, factor(Y_3) ~ ., data=train, kernel= "polynomial", ranges = list(
  cost = c(0.1, 1, 10, 100, 1000),
  gamma = c(0.5, 1, 2, 3, 4)
))
summary(tune_out_svm_polynomial)
#want cost to be 1000 and gamma to be 4
svm_poly <- svm(factor(Y_3) ~ . , data=train, kernel = "polynomial", gamma = 4, cost = 1000, degree=5)
plot(svm_poly, train) #fascinating
table(predict = predict(svm_poly,test), truth = test$Y_3)
#error rate 3/20 = 15%

#support vector machine with radial kernel
tune_out_svm_radial <- tune(svm, factor(Y_3) ~ ., data=train, kernel= "radial", ranges = list(
  cost = c(0.1, 1, 10, 100, 1000),
  gamma = c(0.5, 1, 2, 3, 4)
))
summary(tune_out_svm_radial)
#want cost to be 10 and gamma to be 1
svm_radial <- svm(factor(Y_3) ~ . , data=train, kernel = "radial", gamma = 1, cost = 10)
plot(svm_radial, train) #fascinating
table(predict = predict(svm_radial,test), truth = test$Y_3)
#error rate 2/20 = 10% 
```

The SVM with radial kernel performs best on my data, as backed up by the plots and it having the lowest error rate of 10%.

## Chapter 9, Exercise 5

#### (a)

```{r}
#generate data set
set.seed(1500)
x1 <- runif(500) - 0.5
x2 <- runif(500) - 0.5
y <- 1*(x1^2 - x2^2 > 0)
data_95 <- data.frame(x1,x2,factor(y))
```

#### (b)

```{r}
#plot data
ggplot(data_95, aes(x=x1,y=x2)) + geom_point(aes(color=factor(y))) 
```

#### (c)

```{r}
#create train and test
set.seed(8383)
tr_sample <- sample(1:500,size=500*.8,replace=F)
train_95 <- data_95[tr_sample,]
test_95 <- data_95[-tr_sample,]

#binomial family glm
log_reg <- glm(factor.y. ~ .,data = train_95,family="binomial")
```

#### (d)

```{r}
#predict on training data
preds_95d <- predict(log_reg, data=train_95,type="response")
preds_95d <- ifelse(preds_95d > 0.5,1,0)
pred_df <- data.frame(train_95$x1,train_95$x2,preds_95d)

#plot
ggplot(pred_df, aes(x=`train_95.x1`,y=`train_95.x2`)) + geom_point(aes(color=preds_95d)) 
```

#### (e)

```{r}
#fit model
log_reg_2 <- glm(factor.y. ~ x1**2 + x1*x2 + x1**5 + exp(x1),data = train_95,family="binomial")
```

#### (f)

```{r}
#predict on training data
preds_95e <- predict(log_reg_2, data=train_95,type="response")
preds_95e <- ifelse(preds_95e > 0.5,1,0)
pred_df95e <- data.frame(train_95$x1,train_95$x2,preds_95e)

#plot
ggplot(pred_df95e, aes(x=`train_95.x1`,y=`train_95.x2`)) + geom_point(aes(color=preds_95e)) 
```

#### (g)

```{r}
#support vector classifier
svc_95 <- svm(factor.y. ~ ., data = train_95, kernel = "linear")
set.seed(19)
tune.out.svc.95 <- tune(svm, factor.y.~.,data=train_95, kernel = "linear", ranges = list(cost = c(0.001, 0.01, 0.1, 1, 5, 10, 100)))
summary(tune.out.svc.95)
#we want cost to be 5
best_svc_95 <- svm(factor.y. ~ ., data = train_95, kernel = "linear", cost=5)
summary(best_svc_95)
#predict
preds_95g <- predict(best_svc_95,train_95)
#preds data frame
preds_df95g <- data.frame(train_95$x1,train_95$x2,preds_95g)
ggplot(preds_df95g, aes(x=`train_95.x1`,y=`train_95.x2`)) + geom_point(aes(color=preds_95g))  
```

#### (h)

```{r}
#support vector machine with polynomial kernel
set.seed(9934)
tune_out_svm_polynomial_95 <- tune(svm, factor.y. ~ ., data=train_95, kernel= "polynomial", ranges = list(
  cost = c(0.1, 1, 10),
  gamma = c(0.5, 1, 2, 3)
))
summary(tune_out_svm_polynomial_95)
#want cost to be 1 and gamma to be 1
svm_poly_95 <- svm(factor.y. ~ . , data=train_95, kernel = "polynomial", gamma = 1, cost = 1, degree=4)
poly_95_preds <- predict(svm_poly_95,train_95)

#preds data frame
preds_df95h <- data.frame(train_95$x1,train_95$x2,poly_95_preds)
ggplot(preds_df95g, aes(x=`train_95.x1`,y=`train_95.x2`)) + geom_point(aes(color=poly_95_preds))  

table(poly_95_preds,train_95$factor.y.)
#error rate 15/400 = 3.75%
```

#### (i)

Regular logistic regression without using non-linear predictors gives relatively similar predictions to a support vector classifier, but a SVM gives different predictions than a logistic regression when non-linear predictors are introduced. The SVM predictions, when choosing the correct degree, are much closer to the true values (just based on the plots) than the predictions from the other methods are.

## Chapter 9, Exercise 8

```{r}
#load data
require(ISLR2)
OJ <- OJ
```

#### (a)

```{r}
#create train and test sets
set.seed(0705)
train_sample_98 <- sample(1:1070, size = 800, replace = F)
train_98 <- OJ[train_sample_98,]
test_98 <- OJ[-train_sample_98,]
```

#### (b)

```{r}
set.seed(2000)
svc_98 <- svm(Purchase ~ ., data = train_98, kernel = "linear", cost= 0.01)
summary(svc_98)
```

A linear kernel was used with cost = 0.01 and there were 437 support vectors, 218 in one class and 219 in the other. There were two classes, with levels CH and MM.

#### (c) 

```{r}
#training error rate
train_preds_98 <- predict(svc_98,train_98)
table(train_preds_98, train_98$Purchase)
(55+80)/800 #16.9% error rate

#test error rate
test_preds_98 <- predict(svc_98,test_98)
table(test_preds_98, test_98$Purchase)
(25+23)/270 #17.8% error rate
```

The training error rate is 16.9% and the testing error rate is 17.8%.

#### (d)

```{r}
#tune for cost
set.seed(19876)
tune.out.svc.98 <- tune(svm, Purchase~.,data=train_98, kernel = "linear", ranges = list(cost = c(0.01,0.1,1:10)))
summary(tune.out.svc.98) #optimal cost is 7
```

#### (e)

```{r}
#create new support vector classifier
set.seed(2001)
svc_98e <- svm(Purchase ~ ., data = train_98, kernel = "linear", cost= 7)
summary(svc_98e)

#training error rate
train_preds_98e <- predict(svc_98e,train_98)
table(train_preds_98e, train_98$Purchase)
(56+71)/800 #15.9% error rate

#test error rate
test_preds_98e <- predict(svc_98e,test_98)
table(test_preds_98e, test_98$Purchase)
(30+24)/270 #20% error rate
```

The training error rate is 15.9% and the testing error rate is 20%.

#### (f)

```{r}
#repeat part b
set.seed(838)
svm_radial_98 <- svm(Purchase ~ . , data=train_98, kernel = "radial", cost = 0.01)
summary(svm_radial_98)
#A radial kernel was used with cost = 0.01  and there were 630 support vectors, 314 in one class and 316 in the other. There were two classes, with levels CH and MM.

#repeat part c
#training error rate
train_preds_98fc <- predict(svm_radial_98,train_98)
table(train_preds_98fc, train_98$Purchase)
(314)/800 #39.2% error rate
#test error rate
test_preds_98fc <- predict(svm_radial_98,test_98)
table(test_preds_98fc, test_98$Purchase)
(103)/270 #38.1% error rate

#repeat part d
set.seed(8643)
tune_out_svm_radial_98fd <- tune(svm, Purchase ~ ., data=train_98, kernel= "radial", ranges = list(
  cost = c(0.01,0.1,1:10)
))
summary(tune_out_svm_radial_98fd)
#want cost to be 1

#repeat part e
set.seed(8384)
svm_radial_98fe <- svm(Purchase ~ . , data=train_98, kernel = "radial", cost = 1)
summary(svm_radial_98fe)
#training error rate
train_preds_98fe <- predict(svm_radial_98fe,train_98)
table(train_preds_98fe, train_98$Purchase)
(43+77)/800 #15% error rate
#test error rate
test_preds_98fe <- predict(svm_radial_98fe,test_98)
table(test_preds_98fe, test_98$Purchase)
(20+22)/270 #15.6% error rate
```

#### (g)

```{r}
#repeat part b
set.seed(55667)
svm_poly_98gb <- svm(Purchase ~ . , data=train_98, kernel = "polynomial", cost = 0.01, degree=2)
summary(svm_poly_98gb)
#A polynomial kernel was used with cost = 0.01 and degree = 2 and there were 633 support vectors, 314 in one class and 319 in the other. There were two classes, with levels CH and MM.

#repeat part c
#training error rate
train_preds_98gc <- predict(svm_poly_98gb,train_98)
table(train_preds_98gc, train_98$Purchase)
(314)/800 #39.2% error rate
#test error rate
test_preds_98gc <- predict(svm_poly_98gb,test_98)
table(test_preds_98gc, test_98$Purchase)
(103)/270 #38.1% error rate

#repeat part d
set.seed(86438)
tune_out_svm_poly_98gd <- tune(svm, Purchase ~ ., data=train_98, kernel= "polynomial",degree=2, ranges = list(
  cost = c(0.01,0.1,1:10)
))
summary(tune_out_svm_poly_98gd)
#want cost to be 5

#repeat part e
set.seed(838437)
svm_poly_98ge <- svm(Purchase ~ . , data=train_98, kernel = "polynomial", cost = 5, degree=2)
summary(svm_poly_98ge)
#training error rate
train_preds_98ge <- predict(svm_poly_98ge,train_98)
table(train_preds_98ge, train_98$Purchase)
(35+80)/800 #14.4% error rate
#test error rate
test_preds_98ge <- predict(svm_poly_98ge,test_98)
table(test_preds_98ge, test_98$Purchase)
(15+28)/270 #15.9% error rate
```

#### (h)

The support vector machine with radial kernel and cost = 1 performed the best on the test data with an error rate of 15.6%.
