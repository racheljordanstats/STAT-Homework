---
title: "Predictive Analytics HW 7"
author: "Rachel Jordan"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Chapter 9, Exercise 3

#### (a)

```{r}
#enter data
X1 <- c(3,2,4,1,2,4,4)
X2 <- c(4,2,4,4,1,3,1)
Y <- c("Red","Red","Red","Red","Blue","Blue","Blue")
toy <- as.data.frame(cbind(X1,X2,Y))
toy$Y <- factor(toy$Y,levels=c("Red","Blue"))

#create "sketch"
require(tidyverse)
ggplot(toy, aes(x=X1, y=X2)) + geom_point(aes(color=Y))
```

#### (b)

Visually I can tell that this line should be X2 = -0.5 + X1 which is the same as 0.5 - X1 + X2 = 0.

```{r}
#sketch
ggplot(toy, aes(x=X1, y=X2)) + geom_point(aes(color=Y)) + geom_abline(slope=1,intercept=-0.5)
```

#### (c)

Classify to red if 0.5 - X1 + X2 \> 0 and classify to blue otherwise. B0 = 0.5, B1 = -1, and B2 = 1.

#### (d)

The margins are created by using -1 and 0 as the intercept. The resulting lines will hit the support vectors.

```{r}
#sketch
ggplot(toy, aes(x=X1, y=X2)) + geom_point(aes(color=Y)) + geom_abline(slope=1,intercept=-0.5) + geom_abline(slope=1, intercept=0, linetype="dotted") + geom_abline(slope=1, intercept=-1,linetype="dotted")
```

#### (e)

The support vectors are the points (2,2), (2,1), (4,4), and (4,3).

#### (f)

The 7th observation, (4,1), is not a support vector and would therefore not affect the maximal marginal hyperplane if moved, UNLESS it was moved to be within the margins or across the hyperplane.

#### (g)

```{r}
#sketch
ggplot(toy, aes(x=X1, y=X2)) + geom_point(aes(color=Y)) + geom_abline(slope=1.1,intercept=-0.5)
```

This is not the optimal separating hyperplane. The equation is X2 = -0.5 + 1.1X1, which is equivalent to 0.5 - 1.1X1 + X2 = 0.

#### (h)

```{r}
#add extra point
X1_2 <- c(3,2,4,1,2,4,4,2)
X2_2 <- c(4,2,4,4,1,3,1,3)
Y_2 <- c("Red","Red","Red","Red","Blue","Blue","Blue","Blue")
toy_2 <- as.data.frame(cbind(X1_2,X2_2,Y_2))
toy_2$Y_2 <- factor(toy_2$Y_2,levels=c("Red","Blue"))

#sketch
ggplot(toy_2, aes(x=X1_2, y=X2_2)) + geom_point(aes(color=Y_2))
```

## Chapter 9, Exercise 4

```{r}
#generate data
set.seed(1354)
X1_3 <- rnorm(100)
X2_3 <- runif(100, min = 0, max = 4)
Y_3 <- ifelse(X1_3**2 + X2_3**2 <= 4, 0, 1)
df <- as.data.frame(cbind(X1_3, X2_3, Y_3))

#plot data
ggplot(df, aes(x=X1_3, y=X2_3)) + geom_point(aes(color=Y_3))

#show that in this setting, a support vector machine with polynomial kernal (with degree greater than 1) or a radial kernel will outperform a support vector classifier on the training data

#create train and test
set.seed(1356)
train_sample <- sample(1:100,size=80,replace=F)
train <- df[train_sample,]
test <- df[-train_sample,]

#support vector classifier
require(e1071)
svc <- svm(factor(Y_3) ~ ., data = train, kernel = "linear")
summary(svc)
tune.out.svc <- tune(svm, factor(Y_3) ~ .,data=df, kernel = "linear", ranges = list(cost = c(0.001, 0.01, 0.1, 1, 5, 10, 100)))
summary(tune.out.svc)
#we want cost to be 1
best_svc <- svm(factor(Y_3) ~ ., data = train, kernel = "linear", cost=1)
summary(best_svc)
#error rate
table(predict = (predict(best_svc,test)), truth = test$Y_3)
#error rate is 3/20 = 15%
plot(best_svc,train) #not too shabby

#support vector machine with polynomial kernel
tune_out_svm_polynomial <- tune(svm, factor(Y_3) ~ ., data=train, kernel= "polynomial", ranges = list(
  cost = c(0.1, 1, 10, 100, 1000),
  gamma = c(0.5, 1, 2, 3, 4)
))
summary(tune_out_svm_polynomial)
#want cost to be 1000 and gamma to be 4
svm_poly <- svm(factor(Y_3) ~ . , data=train, kernel = "polynomial", gamma = 4, cost = 1000, degree=5)
plot(svm_poly, train) #fascinating
table(predict = predict(svm_poly,test), truth = test$Y_3)
#error rate 3/20 = 15%

#support vector machine with radial kernel
tune_out_svm_radial <- tune(svm, factor(Y_3) ~ ., data=train, kernel= "radial", ranges = list(
  cost = c(0.1, 1, 10, 100, 1000),
  gamma = c(0.5, 1, 2, 3, 4)
))
summary(tune_out_svm_radial)
#want cost to be 10 and gamma to be 1
svm_radial <- svm(factor(Y_3) ~ . , data=train, kernel = "radial", gamma = 1, cost = 10)
plot(svm_radial, train) #fascinating
table(predict = predict(svm_radial,test), truth = test$Y_3)
#error rate 2/20 = 10% 
```

The SVM with radial kernel performs best on my data, as backed up by the plots and it having the lowest error rate of 10%.

## Chapter 9, Exercise 5

#### (a)

```{r}
#generate data set
set.seed(1500)
x1 <- runif(500) - 0.5
x2 <- runif(500) - 0.5
y <- 1*(x1^2 - x2^2 > 0)
data_95 <- data.frame(x1,x2,factor(y))
```

#### (b)

```{r}
#plot data
ggplot(data_95, aes(x=x1,y=x2)) + geom_point(aes(color=factor(y))) 
```

#### (c)

```{r}
#create train and test
set.seed(8383)
tr_sample <- sample(1:500,size=500*.8,replace=F)
train_95 <- data_95[tr_sample,]
test_95 <- data_95[-tr_sample,]

#binomial family glm
log_reg <- glm(factor.y. ~ .,data = train_95,family="binomial")
```

#### (d)

```{r}
#predict on training data
preds_95d <- predict(log_reg, data=train_95,type="response")
preds_95d <- ifelse(preds_95d > 0.5,1,0)
pred_df <- data.frame(train_95$x1,train_95$x2,preds_95d)

#plot
ggplot(pred_df, aes(x=`train_95.x1`,y=`train_95.x2`)) + geom_point(aes(color=preds_95d)) 
```

#### (e)

```{r}
#fit model
log_reg_2 <- glm(factor.y. ~ x1**2 + x1*x2 + x1**5 + exp(x1),data = train_95,family="binomial")
```

#### (f)

```{r}
#predict on training data
preds_95e <- predict(log_reg_2, data=train_95,type="response")
preds_95e <- ifelse(preds_95e > 0.5,1,0)
pred_df95e <- data.frame(train_95$x1,train_95$x2,preds_95e)

#plot
ggplot(pred_df95e, aes(x=`train_95.x1`,y=`train_95.x2`)) + geom_point(aes(color=preds_95e)) 
```

#### (g)

```{r}
#support vector classifier
svc_95 <- svm(factor.y. ~ ., data = train_95, kernel = "linear")
set.seed(19)
tune.out.svc.95 <- tune(svm, factor.y.~.,data=train_95, kernel = "linear", ranges = list(cost = c(0.001, 0.01, 0.1, 1, 5, 10, 100)))
summary(tune.out.svc.95)
#we want cost to be 5
best_svc_95 <- svm(factor.y. ~ ., data = train_95, kernel = "linear", cost=5)
summary(best_svc_95)
#predict
preds_95g <- predict(best_svc_95,train_95)
#preds data frame
preds_df95g <- data.frame(train_95$x1,train_95$x2,preds_95g)
ggplot(preds_df95g, aes(x=`train_95.x1`,y=`train_95.x2`)) + geom_point(aes(color=preds_95g))  
```

#### (h)

```{r}
#support vector machine with polynomial kernel
set.seed(9934)
tune_out_svm_polynomial_95 <- tune(svm, factor.y. ~ ., data=train_95, kernel= "polynomial", ranges = list(
  cost = c(0.1, 1, 10),
  gamma = c(0.5, 1, 2, 3)
))
summary(tune_out_svm_polynomial_95)
#want cost to be 1 and gamma to be 1
svm_poly_95 <- svm(factor.y. ~ . , data=train_95, kernel = "polynomial", gamma = 1, cost = 1, degree=4)
poly_95_preds <- predict(svm_poly_95,train_95)

#preds data frame
preds_df95h <- data.frame(train_95$x1,train_95$x2,poly_95_preds)
ggplot(preds_df95g, aes(x=`train_95.x1`,y=`train_95.x2`)) + geom_point(aes(color=poly_95_preds))  

table(poly_95_preds,train_95$factor.y.)
#error rate 15/400 = 3.75%
```

#### (i)

Regular logistic regression without using non-linear predictors gives relatively similar predictions to a support vector classifier, but a SVM gives different predictions than a logistic regression when non-linear predictors are introduced. The SVM predictions, when choosing the correct degree, are much closer to the true values (just based on the plots) than the predictions from the other methods are.
